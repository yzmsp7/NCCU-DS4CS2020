{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparkAmplify_hoemwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPBJqsOjy7ZqxM7m1PfznRc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yzmsp7/NCCU-DS4CS2020/blob/master/sparkAmplify_hoemwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PijOSYDL7Rva",
        "colab_type": "text"
      },
      "source": [
        "# SparkAmplify Code Challenge\n",
        "\n",
        "### Description\n",
        "\n",
        "We have 15,000 articles in our hands which are from N categories.\n",
        "\n",
        "### Problem\n",
        "\n",
        "1. Please plot the position of the articles in two dimensional space and cluster the articles in several groups. Then, you need to pick the representative article toward each group. (note: you have to explain the reason of picking group number)\n",
        "2. Based on the 1, what are the top-5 keywords of each representative article.\n",
        "3. Based on the 1 and 2, please rank the top-10 document order with the top-5 keywords from the representative article toward each group.\n",
        "4. Based on the 1 and 2, please rank the top-10 document order with the content from the representative article toward each group.\n",
        "5. Please combine both methods you applied in the 3 and 4 to output the similar format result.\n",
        "\n",
        "\n",
        "### EDA\n",
        "\n",
        "1. Missing article name (na): 397\n",
        "2. Different article name but same contents. e.g. 1259184-1259200: 160\n",
        "3. Aritcle name contains the different language e.g russian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8scKt48T7PDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PnsF7bh7dSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles = pd.read_csv(\"/content/articles_raw.csv\")\n",
        "articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgtN71BA7ySd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dup = articles[articles.duplicated(subset=['text'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp7psnlB93z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dup_id = dup.id.to_list()\n",
        "na_id = articles[articles.title.isna()].id.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9FZ0ZZx-K7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles_uniq = articles[~articles.id.isin(na_id+dup_id)]\n",
        "print(\"legnth: \", len(articles_uniq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWfdt4pq-9UF",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "spacy: https://allenai.github.io/scispacy/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CLqrHIJyolr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rrA0QXDlcXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "import string\n",
        "import pyLDAvis.gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5GlkrexmnAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VidtDGBMlhfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = r'\\b[^\\d\\W]+\\b'\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "en_stop = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF5WjWezw6z-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizeText(data):\n",
        "    raw = str(data).lower()\n",
        "    tokens = tokenizer.tokenize(raw)\n",
        "\n",
        "    # remove stop words from tokens\n",
        "    stopped_tokens = [raw for raw in tokens if not raw in en_stop]\n",
        "    \n",
        "    # lemmatize tokens\n",
        "    lemma_tokens = [lemmatizer.lemmatize(tokens) for tokens in stopped_tokens]\n",
        "    \n",
        "    # remove word containing only single char\n",
        "    new_lemma_tokens = [raw for raw in lemma_tokens if not len(raw) == 1]\n",
        "    \n",
        "    return new_lemma_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3ZuX3BVxXbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = []\n",
        "for row in articles_uniq.itertuples():\n",
        "    texts.append(normalizeText(row[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cs_GFeT-o8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nlp = en_core_web_sm.load()\n",
        "\n",
        "# doc = nlp(articles_uniq.text[0])\n",
        "# spacy_words = [token.text for token in doc]\n",
        "# print(f\"Tokenized words: {spacy_words}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msFHyRmpA9eK",
        "colab_type": "text"
      },
      "source": [
        "### Document Clustering - TF-IDF + KMeans\n",
        "\n",
        "Dimension reduction to vis: pca v.s. t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sOd5WWdBBJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INyBxQgKBR_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "articles_uniq['nor_text'] = texts\n",
        "articles_uniq['nor_text'] = articles_uniq['nor_text'].copy().apply(lambda x: ' '.join(x))\n",
        "tfidf = TfidfVectorizer(\n",
        "    min_df = 5,\n",
        "    max_df = 0.95,\n",
        "    max_features = 8000,\n",
        "    stop_words = 'english'\n",
        ")\n",
        "tfidf.fit(articles_uniq.nor_text)\n",
        "contents = tfidf.transform(articles_uniq.nor_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGkgLwyhBaXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_optimal_clusters(data, max_k):\n",
        "    iters = range(2, max_k+1, 2)\n",
        "    \n",
        "    sse = []\n",
        "    for k in iters:\n",
        "        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n",
        "        print('Fit {} clusters'.format(k))\n",
        "        \n",
        "    f, ax = plt.subplots(1, 1)\n",
        "    ax.plot(iters, sse, marker='o')\n",
        "    ax.set_xlabel('Cluster Centers')\n",
        "    ax.set_xticks(iters)\n",
        "    ax.set_xticklabels(iters)\n",
        "    ax.set_ylabel('SSE')\n",
        "    ax.set_title('SSE by Cluster Center Plot')\n",
        "    \n",
        "find_optimal_clusters(contents, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAmdIafjBfZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clusters = MiniBatchKMeans(n_clusters=8, init_size=1024, batch_size=2048, random_state=20).fit_predict(contents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-M928c_BkMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_tsne_pca(data, labels):\n",
        "    max_label = max(labels)\n",
        "    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=False)\n",
        "    \n",
        "    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n",
        "    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))\n",
        "    \n",
        "    \n",
        "    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)\n",
        "    label_subset = labels[max_items]\n",
        "    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]\n",
        "    \n",
        "    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n",
        "    ax[0].set_title('PCA Cluster Plot')\n",
        "    \n",
        "    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n",
        "    ax[1].set_title('TSNE Cluster Plot')\n",
        "    \n",
        "plot_tsne_pca(contents, clusters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJqmnxnVBkkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def get_top_keywords(data, clusters, labels, n_terms):\n",
        "    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n",
        "    \n",
        "    for i,r in df.iterrows():\n",
        "        print('\\nCluster {}'.format(i))\n",
        "        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n",
        "            \n",
        "get_top_keywords(contents, clusters, tfidf.get_feature_names(), 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udjviPF8Bo6v",
        "colab_type": "text"
      },
      "source": [
        "### Document Clustering - LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNHRseJZFV2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        " \n",
        "def print_topics(model, vectorizer, n_top_words):\n",
        "    words = vectorizer.get_feature_names()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"\\nTopic #%d:\" % topic_idx)\n",
        "        print(\" \".join([words[i]\n",
        "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "        \n",
        "number_topics = 8\n",
        "number_words = 5\n",
        "# Create and fit the LDA model\n",
        "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
        "lda.fit(contents)\n",
        "# Print the topics found by the LDA model\n",
        "print(\"Topics found via LDA:\")\n",
        "print_topics(lda, tfidf, number_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS7Ctc5vycRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# turn our tokenized documents into a id <-> term dictionary\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "# convert tokenized documents into a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQqdGg_rygJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=8, id2word = dictionary, passes=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQY6Zbhm1aKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLIxIJJUXIsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "twords = {}\n",
        "for i, tp in enumerate(ldamodel.show_topics(num_words=5)):\n",
        "    twords[i] = re.sub('[^A-Za-z ]+', '', tp[1]).split('  ')\n",
        "\n",
        "twords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbdJr6trGtnh",
        "colab_type": "text"
      },
      "source": [
        "### Rank top 10 document with top-5 keywords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6XdQ2VRUs2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import similarities\n",
        "\n",
        "index = similarities.MatrixSimilarity(ldamodel[corpus])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b88dAMQST-nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "playrVdAGPZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_similarity(lda, query_vector):\n",
        "    index = similarities.MatrixSimilarity(lda[corpus])\n",
        "    sims = index[query_vector]\n",
        "    return sims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3AO17k_bQWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Top 10 documents with top-5 keywords\")\n",
        "for topic_i in range(len(twords)):\n",
        "    print(\"topic {} 's keywords: {}\".format(topic_i, twords[topic_i]))\n",
        "    query = ldamodel[dictionary.doc2bow(twords[topic_i])]\n",
        "    sims = get_similarity(ldamodel, query)\n",
        "    sims = sorted(enumerate(sims), key=lambda item: -item[1]) # ranking\n",
        "    \n",
        "    for i in range(10):\n",
        "        target = articles_uniq.iloc[sims[i][0], :]\n",
        "        print(\"id: {} / title: {}\".format(target['id'], target['title']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ixUJpYb_t-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}